{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5468be7-7b35-49c7-a24b-c66e0bc97fb5",
   "metadata": {},
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6635d25c-bcc5-43be-aa8f-2a74c7b82542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a259fc38-229b-4551-986d-32f540d619d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b6da2-bac0-4eff-8ce9-b221cfceedc6",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "774bdead-0f1a-4def-bf6a-972b61e141d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/train.txt'\n",
    "DEV_PATH = 'data/val.txt'\n",
    "TEST_PATH = 'data/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd77046-b89c-45e4-b46a-86ef202bb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_txt(path):\n",
    "    QandA = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file):\n",
    "            #print(line)\n",
    "            if line != \"\\n\":\n",
    "                splitted = line.split(\"|||\")\n",
    "                sentences = splitted[0]\n",
    "                question  = splitted[1]\n",
    "                answer    = re.sub(\"\\n\",\"\",splitted[2])\n",
    "                answer = r\" \".join(answer.split())\n",
    "                QandA.append((sentences, question, answer))\n",
    "    return QandA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a560c18e-58af-471f-9e7a-75bd1f2bec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99820it [00:01, 54500.25it/s]\n",
      "13893it [00:00, 50494.85it/s]\n",
      "27248it [00:00, 57604.32it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = read_data_from_txt(TRAIN_PATH)\n",
    "del train_data[51641] #51641報錯\n",
    "valid_data = read_data_from_txt(DEV_PATH)\n",
    "test_data  = read_data_from_txt(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f7afa-b860-4fa2-abf9-632b90e01d21",
   "metadata": {},
   "source": [
    "# check answer 是否在文章內"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeaf0a55-f17a-435d-a345-41443cdcc7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 99819/99819 [00:17<00:00, 5568.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every answer is in context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 13893/13893 [00:02<00:00, 5866.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every answer is in context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def check_answer(data):\n",
    "    index = []\n",
    "    for sentences, question, answer in tqdm(data):\n",
    "        ans = \"FALSE\"\n",
    "        sentences = re.findall(r'<s>(.*?)</s>', sentences)\n",
    "        for sentence in sentences:\n",
    "            try:\n",
    "                if answer in sentence:\n",
    "                    ans = \"TRUE\" \n",
    "            except:\n",
    "                continue\n",
    "        if ans == \"FALSE\":\n",
    "                index.append(i)\n",
    "    if len(index) == 0:\n",
    "        print(\"Every answer is in context.\")\n",
    "check_answer(train_data)\n",
    "check_answer(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c8eb4-5cb1-4ab4-90fe-b322e536505c",
   "metadata": {},
   "source": [
    "# 用tf-idf 選取和問題最相近的句子，包含答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d428cfe-086d-41f7-85c3-3347058aa96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_articles(query, docs, k=2):\n",
    "\n",
    "    # Initialize a vectorizer that removes English stop words\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words='english')\n",
    "\n",
    "    # Create a corpus of query and documents and convert to TFIDF vectors\n",
    "    query_and_docs = [query] + docs\n",
    "    matrix = vectorizer.fit_transform(query_and_docs)\n",
    "\n",
    "    # Holds our cosine similarity scores\n",
    "    scores = []\n",
    "\n",
    "    # The first vector is our query text, so compute the similarity of our query against all document vectors\n",
    "    for i in range(1, len(query_and_docs)):\n",
    "        scores.append(cosine_similarity(matrix[0], matrix[i])[0][0])\n",
    "\n",
    "    # Sort list of scores and return the top k highest scoring documents\n",
    "    sorted_list = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    top_doc_indices = [x[0] for x in sorted_list[:k]]\n",
    "    top_docs = [docs[x] for x in top_doc_indices]\n",
    "  \n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51936bd0-183f-4815-9522-1c52d59eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m lebowski sweater replica jun 17 , 2013 history last 8 years life , galileo house arrest espousing man 's theory copernicus espn 's top 10 time \u001b[0m\n",
      "\u001b[95m images knickerless women jun 17 , 2013 history last 8 years life , galileo house arrest espousing man 's theory copernicus espn 's top 10 time \u001b[0m\n",
      "\u001b[1m last 8 years life , galileo house arrest espousing man 's theory \u001b[1m\n",
      "\u001b[94mcopernicus\u001b[4m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test example\n",
    "for sentences, question, answer in train_data[0:1]:\n",
    "    contained_answer_sentence = []\n",
    "    sentences = re.findall(r'<s>(.*?)</s>', sentences)\n",
    "    for sentence in sentences:\n",
    "        if answer in sentence:\n",
    "            contained_answer_sentence.append(sentence)\n",
    "    doc = get_top_k_articles(question, contained_answer_sentence)\n",
    "    for i in doc:\n",
    "        print(bcolors.HEADER+i+bcolors.ENDC)\n",
    "    print(bcolors.BOLD+question+bcolors.BOLD)\n",
    "    print(bcolors.OKBLUE+answer+bcolors.UNDERLINE)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6bb1950-7b7b-42fe-9395-1d22ee453b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 99819/99819 [18:43<00:00, 88.84it/s]\n",
      "100%|████████████████████████████████████| 13893/13893 [02:11<00:00, 105.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" lebowski sweater replica jun 17 , 2013 history last 8 years life , galileo house arrest espousing man 's theory copernicus espn 's top 10 time .  images knickerless women jun 17 , 2013 history last 8 years life , galileo house arrest espousing man 's theory copernicus espn 's top 10 time \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_topk_tfidf_sentence(data):\n",
    "    contexts = []\n",
    "    for sentences, question, answer in tqdm(data):\n",
    "        contained_answer_sentence = []\n",
    "        sentences = re.findall(r'<s>(.*?)</s>', sentences)\n",
    "        for sentence in sentences:\n",
    "            if answer in sentence:\n",
    "                contained_answer_sentence.append(sentence)\n",
    "        doc = get_top_k_articles(question, contained_answer_sentence)\n",
    "        context = \". \".join(doc)\n",
    "        contexts.append(context)\n",
    "    return contexts\n",
    "train_contexts = get_topk_tfidf_sentence(train_data)\n",
    "val_contexts =   get_topk_tfidf_sentence(valid_data)\n",
    "train_contexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62860538-5802-432f-91f3-84e05c880dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 27248/27248 [11:18<00:00, 40.13it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_topk_tfidf_sentence_test(data):\n",
    "    contexts = []\n",
    "    for sentences, question, answer in tqdm(data):\n",
    "        contained_answer_sentence = []\n",
    "        sentences = re.findall(r'<s>(.*?)</s>', sentences)\n",
    "        for sentence in sentences:\n",
    "            contained_answer_sentence.append(sentence)\n",
    "        doc = get_top_k_articles(question, contained_answer_sentence)\n",
    "        context = \". \".join(doc)\n",
    "        contexts.append(context)\n",
    "    return contexts\n",
    "test_contexts = get_topk_tfidf_sentence_test(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2827427-ea5a-4f42-861f-d2f232ffd0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99819\n",
      "99819\n",
      "13893\n",
      "13893\n",
      "27248\n",
      "27248\n"
     ]
    }
   ],
   "source": [
    "print(len(train_contexts))\n",
    "print(len(train_data))\n",
    "print(len(val_contexts))\n",
    "print(len(valid_data))\n",
    "print(len(test_contexts))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45e935-3121-4efc-9239-2a6d8ddeae7b",
   "metadata": {},
   "source": [
    "# 轉換成Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b22824-930b-4364-b439-1d6949bf00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(data, contexts):\n",
    "    ques = []\n",
    "    ans= []\n",
    "    for sentences, question, answer in data:\n",
    "        ques.append(question)\n",
    "        ans.append(answer)\n",
    "    return pd.DataFrame({'sentences':contexts,'question':ques,'answer':ans})\n",
    "train_data = convert_to_dataframe(train_data, train_contexts)\n",
    "valid_data = convert_to_dataframe(valid_data, val_contexts)\n",
    "test_data =  convert_to_dataframe(test_data, test_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0536454f-cb60-4bfe-b32c-78bbb9c9b7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43366</th>\n",
       "      <td>nanjing built tomb emperor hongwu , first emperor ming dynasty , 1383 nanjing museum famous biggest collection china .  ming tombs wikipedia ming tombs collection mausoleums built emperors ming dynasty china first ming emperor 's tomb located near capital nanjing xiaoling tomb first ming emperor , hongwu emperor , located near capital nanjing second emperor , jianwen emperor ,</td>\n",
       "      <td>tomb hongwu , first emperor famous dynasty , nanjing , china</td>\n",
       "      <td>ming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_random_elements(dataset, num_examples=1):\n",
    "    df = dataset.sample(n = num_examples)\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "show_random_elements(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1445d-7837-4526-a51e-a1b46f8d37a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d5702f7-78e5-4352-8b60-e6237e434409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 17:15:21.717978: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-14 17:15:21.913237: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-14 17:15:22.451791: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:\n",
      "2022-11-14 17:15:22.451889: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:\n",
      "2022-11-14 17:15:22.451899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "999321da-7300-4195-b283-62521c2550eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokennize_list(data):\n",
    "    data_question = data['question'].tolist()\n",
    "    data_context = data['sentences'].tolist()\n",
    "    return data_question, data_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8df9f390-6399-4f0b-8df8-276a12fe6da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_question, train_context = get_tokennize_list(train_data)\n",
    "valid_question, valid_context = get_tokennize_list(valid_data)\n",
    "test_question,  test_context  = get_tokennize_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad7dd308-7f92-46e7-a019-5a22fa1c1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_question, train_context, truncation=True, padding=True)\n",
    "val_encodings   = tokenizer(valid_question, valid_context, truncation=True, padding=True)\n",
    "test_encodings  = tokenizer(test_question, test_context, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00bd02bf-c0f6-406c-a71c-715ecc6d05ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] last 8 years life, galileo house arrest espousing man's theory [SEP] lebowski sweater replica jun 17, 2013 history last 8 years life, galileo house arrest espousing man's theory copernicus espn's top 10 time. images knickerless women jun 17, 2013 history last 8 years life, galileo house arrest espousing man's theory copernicus espn's top 10 time [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8ab73-cda1-4f54-876a-8a03be778328",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 找答案的start end index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaf0d315-ac97-4b61-905a-1401f3d6a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Start_End_index(data):\n",
    "    data['start'] = [y.index(x) for x,y in zip(data[\"answer\"],data[\"sentences\"])]\n",
    "    data['end']   = [x+len(str(y)) for x,y in zip(data[\"start\"],data[\"answer\"])]\n",
    "get_Start_End_index(train_data)\n",
    "get_Start_End_index(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad5bb017-e851-4498-a1dc-68c266f893a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_test_index(data):\n",
    "    data['start'] = 0\n",
    "    data['end']   = 0\n",
    "set_test_index(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eeadd48-fc3e-48ff-94fa-feee17840650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lebowski sweater replica jun 17 , 2013 histor...</td>\n",
       "      <td>last 8 years life , galileo house arrest espo...</td>\n",
       "      <td>copernicus</td>\n",
       "      <td>113</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jim thorpe bio , stats , results olympics spo...</td>\n",
       "      <td>2 1912 olympian football star carlisle indian...</td>\n",
       "      <td>jim thorpe</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yuma arizona oddities part 3 jul 21 , 2011 yu...</td>\n",
       "      <td>city yuma state record average 4 , 055 hours ...</td>\n",
       "      <td>arizona</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gravestone famous ancestor roger sherman sign...</td>\n",
       "      <td>signer dec indep , framer constitution mass ,...</td>\n",
       "      <td>john adams</td>\n",
       "      <td>176</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014 new york international fringe festival ,...</td>\n",
       "      <td>title aesop fable , insect shared billing gra...</td>\n",
       "      <td>ant</td>\n",
       "      <td>257</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0   lebowski sweater replica jun 17 , 2013 histor...   \n",
       "1   jim thorpe bio , stats , results olympics spo...   \n",
       "2   yuma arizona oddities part 3 jul 21 , 2011 yu...   \n",
       "3   gravestone famous ancestor roger sherman sign...   \n",
       "4   2014 new york international fringe festival ,...   \n",
       "\n",
       "                                            question      answer  start  end  \n",
       "0   last 8 years life , galileo house arrest espo...  copernicus    113  123  \n",
       "1   2 1912 olympian football star carlisle indian...  jim thorpe      1   11  \n",
       "2   city yuma state record average 4 , 055 hours ...     arizona      6   13  \n",
       "3   signer dec indep , framer constitution mass ,...  john adams    176  186  \n",
       "4   title aesop fable , insect shared billing gra...         ant    257  260  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "724fd848-c32e-444b-9f0b-c52dece503f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'copernicus'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['sentences'].iloc[0][113:123]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497799e4-ac87-4d75-b9ef-58e34bafc684",
   "metadata": {},
   "source": [
    "# add token positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1c9cf6e-3f34-4cac-a7fb-4c12eb74ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answer = train_data[['start', 'end']].to_dict('records')\n",
    "valid_answer = valid_data[['start', 'end']].to_dict('records')\n",
    "test_answer  = test_data[['start', 'end']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd79af5-0bf7-4b8a-8da7-f2070c2d3835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copernicus\n",
      "47\n",
      "49\n",
      "copernicus\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(train_data['answer'].iloc[i])\n",
    "a = train_encodings.char_to_token(i, train_answer[i]['start'], 1)\n",
    "print(a)\n",
    "b = train_encodings.char_to_token(i, train_answer[i]['end']-1, 1)\n",
    "print(b)\n",
    "print(tokenizer.decode(train_encodings['input_ids'][i][47:b+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24954ba0-9a6f-420c-8968-875574a11d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['start'],1))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['end']-1,1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "            \n",
    "        shift = 1\n",
    "        while end_positions[-1] is None:\n",
    "            if answers[i]['end'] - shift>=0:\n",
    "                end_positions[-1] = encodings.char_to_token(i, answers[i]['end'] - shift, 1)\n",
    "                shift += 1 \n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    encodings.update({'start': start_positions, 'end': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answer)\n",
    "add_token_positions(val_encodings, valid_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bf1d76e-2865-4a5f-9602-ec4e2b2f50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert char_based_id to token_based_id\n",
    "# Find the corossponding token id after input being tokenized\n",
    "add_token_positions(train_encodings, train_answer)\n",
    "add_token_positions(val_encodings, valid_answer)\n",
    "#add_token_positions(test_encodings, test_answer, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76f41d22-155f-4ccf-8748-6c5082b6aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start', 'end'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4db46f0a-2c1f-4a6c-a116-079bcde29c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "49\n",
      "36\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['start'][0])\n",
    "print(train_encodings['end'][0])\n",
    "print(val_encodings['start'][0])\n",
    "print(val_encodings['end'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c945d874-6b91-4713-862d-97c0d7efb559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oratory\n",
      "oratory\n"
     ]
    }
   ],
   "source": [
    "i=23\n",
    "print(train_data['sentences'].iloc[i][train_answer[i]['start']:train_answer[i]['end']])\n",
    "print(tokenizer.decode(train_encodings['input_ids'][i][train_encodings['start'][i]:train_encodings['end'][i]+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381f5a8-6242-47ed-a3cb-b05c5e3407aa",
   "metadata": {},
   "source": [
    "# convert to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76557abc-b0a1-4f14-9062-1adeb15ba9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            return {key: torch.tensor(value[idx]) for key, value in self.encodings.items()}\n",
    "        except:\n",
    "            print(idx)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1f3a9ae-0f63-418c-b390-ee1197c57d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QADataset(train_encodings)\n",
    "val_dataset   = QADataset(val_encodings)\n",
    "test_dataset  = QADataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5d2e738-5daa-4a8f-af49-1ded38a49528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 22620,  1394,  3003,   117,  1724,  4249,  2247,   117,  1642,\n",
       "          6048, 12200, 22217,  1116,   102,  1714,  6746, 18464,  5752,  2025,\n",
       "          8419,  2158, 22620,  1394,  3003,   117,  1724,  4249,  2247,   117,\n",
       "          1642,  6048, 12200, 22217,  1116,   117, 23123,  1158,  2787,  6394,\n",
       "           119,  1185,  2707, 10615,  1489,   117,  1369,  1270,  7761,  1821,\n",
       "         26237,  1389,  5752, 22620,  1394,  3003,   117,  1724,  4249,  2247,\n",
       "           117,  1642,  6048, 12200, 22217,  1116, 23123,  1158,  2787,  6394,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " 'start': tensor(36),\n",
       " 'end': tensor(38)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674f8be-152b-4b8c-9377-fc6491c55358",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d7b0ddb-6819-4ae5-9446-d18b50cea7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class QAModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"nreimers/MiniLM-L6-H384-uncased\")\n",
    "        self.fc = torch.nn.Linear(384, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
    "        logits = output[0]\n",
    "        out = self.fc(logits)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0acf7597-0e1d-4e17-b6b1-a7640efaa198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set GPU / CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Put model on device\n",
    "model = QAModel().to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084bcc6b-77e5-46a7-a96a-01f320c4dea1",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d7b5768-3a39-4a3e-a260-880fd894ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack data into dataloader by batch\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1515b417-4606-44a5-a2b7-606e9f4a380b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6239\n",
      "869\n",
      "1703\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(valid_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6bd4552-cd8f-4b3d-a25d-5a4345199aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epoch = 10\n",
    "loss_fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcefb50d-c7fb-4c88-af70-b298500bae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(valid_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(valid_loader, leave=True)\n",
    "        for batch_id, batch in enumerate(loop):\n",
    "            input_ids      = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start          = batch['start'].to(device)\n",
    "            end            = batch['end'].to(device)\n",
    "\n",
    "            # model output\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "            start_logits, end_logits = torch.split(outputs, 1, 2)\n",
    "\n",
    "            start_logits = start_logits.squeeze(-1).contiguous()\n",
    "            end_logits   = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "            start_loss = loss_fct(start_logits, start)\n",
    "            end_loss   = loss_fct(end_logits, end)\n",
    "\n",
    "            loss = start_loss + end_loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print('Validation Loss {:.4f}'.format(running_loss / len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c3418-4101-4e76-84c7-8c8fae8f5b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  95%|██████████████████ | 5934/6239 [13:23<00:42,  7.14it/s, loss=1.49]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 1:  91%|████████████████▍ | 5691/6239 [13:14<01:16,  7.12it/s, loss=0.972]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 2:  72%|█████████████▌     | 4463/6239 [10:14<04:10,  7.09it/s, loss=2.25]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 2:  87%|████████████████▌  | 5452/6239 [12:32<01:50,  7.14it/s, loss=1.42]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 3:  68%|████████████▉      | 4267/6239 [09:52<04:36,  7.12it/s, loss=1.15]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 3:  84%|████████████████   | 5262/6239 [12:09<02:16,  7.15it/s, loss=1.24]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 4:  74%|█████████████▉     | 4588/6239 [10:28<03:52,  7.10it/s, loss=1.41]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 5:  68%|████████████▏     | 4232/6239 [09:51<04:42,  7.11it/s, loss=0.563]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 6:  61%|███████████       | 3823/6239 [08:48<05:36,  7.17it/s, loss=0.673]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 7:  39%|███████           | 2438/6239 [05:40<08:47,  7.20it/s, loss=0.573]"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch_id, batch in enumerate(loop):\n",
    "        # reset\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids      = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        start          = batch['start'].to(device)\n",
    "        end            = batch['end'].to(device)\n",
    "\n",
    "        # model output\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        start_logits, end_logits = torch.split(outputs, 1, 2)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits   = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        start_loss = loss_fct(start_logits, start)\n",
    "        end_loss = loss_fct(end_logits, end)\n",
    "        loss = start_loss + end_loss\n",
    "        # calculate loss\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    print('Training Loss {:.4f}'.format(running_loss / len(train_loader)))\n",
    "    evaluate(valid_loader)\n",
    "    \n",
    "torch.save(model.state_dict(),\"model/\" + 'QA_model_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a4c29-17b6-40f2-8123-d04e127aa836",
   "metadata": {},
   "source": [
    "# evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfb31815-0108-4a1c-9883-7b21950ad954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predict_pos = []\n",
    "    sub_output = []\n",
    "\n",
    "    loop = tqdm(test_loader, leave=True)\n",
    "    for batch_id, batch in enumerate(loop):\n",
    "        input_ids      = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "        # model output\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        start_logits, end_logits = torch.split(outputs, 1, 2)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits   = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        start_prdict = torch.argmax(start_logits, 1).cpu().numpy()\n",
    "        end_prdict   = torch.argmax(end_logits, 1).cpu().numpy()\n",
    "\n",
    "        for i in range(len(input_ids)):\n",
    "            predict_pos.append((start_prdict[i].item(), end_prdict[i].item()))\n",
    "            sub = tokenizer.decode(input_ids[i][start_prdict[i]:end_prdict[i]+1])\n",
    "            sub_output.append(sub)\n",
    "    \n",
    "    return sub_output, predict_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e30265c-ebe2-444f-ab51-aa5fc5f73501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 869/869 [00:33<00:00, 25.56it/s]\n"
     ]
    }
   ],
   "source": [
    "sub_output, predict_pos = predict(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5644686c-c4e9-4f66-865e-0f4838acdc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hemingway'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb1604be-f788-4d92-8d6c-ada4a4329d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_token_string(sentence):\n",
    "    # print(sentence)\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        if len(tokens[i]) == 1:\n",
    "            tokens[i] = re.sub(r\"[!\\\"#$%&\\'()*\\+, -.\\/:;<=>?@\\[\\\\\\]^_`{|}~]\", '', tokens[i])\n",
    "    while '' in tokens:\n",
    "        tokens.remove('')\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f18b197-10be-4b7c-afa4-bd2cbb331966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_post_fn(test, sub_output):\n",
    "    sub = []\n",
    "    for i in range(len(test)):\n",
    "\n",
    "        sub_pred = sub_output[i].split()\n",
    "\n",
    "        temp = sub_pred.copy()\n",
    "        if sub_pred is None:\n",
    "            sub_pred = []\n",
    "        else:\n",
    "            for j in range(len(temp)):\n",
    "                if temp[j] == '[SEP]':\n",
    "                    sub_pred.remove('[SEP]')\n",
    "                if temp[j] == '[PAD]':\n",
    "                    sub_pred.remove('[PAD]')\n",
    "\n",
    "        sub.append(' '.join(sub_pred))\n",
    "        \n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71ff3961-2b47-4af0-a55f-5df0c03e6600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>free flashcards authors studystack spain 1959...</td>\n",
       "      <td>spain 1959 , wrote dangerous summer , story r...</td>\n",
       "      <td>hemingway</td>\n",
       "      <td>101</td>\n",
       "      <td>110</td>\n",
       "      <td>hemingway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california facts , map state symbols enchante...</td>\n",
       "      <td>valley 282 feet sea level state lowest point ...</td>\n",
       "      <td>california</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price convenience? atm surcharge debate jul 1...</td>\n",
       "      <td>like banks , many grocery stores dispensing c...</td>\n",
       "      <td>atms</td>\n",
       "      <td>127</td>\n",
       "      <td>131</td>\n",
       "      <td>atms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>steamboat willy classic cartoons pinterest st...</td>\n",
       "      <td>voice mickey mouse steamboat willie</td>\n",
       "      <td>walt disney</td>\n",
       "      <td>135</td>\n",
       "      <td>146</td>\n",
       "      <td>walt disney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eastern europe see section 2 2 6 nation state...</td>\n",
       "      <td>eastern european capital city 2 2 million</td>\n",
       "      <td>bucharest</td>\n",
       "      <td>231</td>\n",
       "      <td>240</td>\n",
       "      <td>bucharest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>us state longest shoreline?? guide humans ala...</td>\n",
       "      <td>6 , 640 miles coast , state longest shoreline</td>\n",
       "      <td>alaska</td>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>north dakota pumps 1 million barrels oil day ...</td>\n",
       "      <td>pumps one million barrels oil day , state</td>\n",
       "      <td>texas</td>\n",
       "      <td>167</td>\n",
       "      <td>172</td>\n",
       "      <td>north dakota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>day day npr hear day day program march 20 , 2...</td>\n",
       "      <td>day day things considered among programs goin...</td>\n",
       "      <td>npr</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>npr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>daffy known voice mel blanc 1937 1989 daffy d...</td>\n",
       "      <td>voice daffy duck first 50 years</td>\n",
       "      <td>mel blanc</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>mel blanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>region 4 russ nelson 's home page glossary ba...</td>\n",
       "      <td>braced framework carrying railroad chasm</td>\n",
       "      <td>trestle</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>russ nelson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>twinkle chubbins astonishing adventures natur...</td>\n",
       "      <td>name laura bancroft , wrote twinkle chubbins ...</td>\n",
       "      <td>l frank baum</td>\n",
       "      <td>198</td>\n",
       "      <td>210</td>\n",
       "      <td>l frank baum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>november 14 , 2008 eagerly crave rhymes wave ...</td>\n",
       "      <td>considered healthiest state 2006 , 's also ho...</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>128</td>\n",
       "      <td>137</td>\n",
       "      <td>minnesota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cia headquarters named president bush cia hea...</td>\n",
       "      <td>headquarters compound langley , virginia name...</td>\n",
       "      <td>cia</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>puss boots shrek wikipedia puss boots fiction...</td>\n",
       "      <td>voiced puss boots shrek 2</td>\n",
       "      <td>antonio banderas</td>\n",
       "      <td>222</td>\n",
       "      <td>238</td>\n",
       "      <td>antonio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>benet , william rose sgeresultat aalborg bibl...</td>\n",
       "      <td>william rose benet pulitzer dust god , brothe...</td>\n",
       "      <td>stephen vincent benet</td>\n",
       "      <td>117</td>\n",
       "      <td>138</td>\n",
       "      <td>stephen vincent benet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>life begins 40 boeing blogs aug 17 , 2007 thi...</td>\n",
       "      <td>boeing manufacturing plant everett world 's l...</td>\n",
       "      <td>washington</td>\n",
       "      <td>151</td>\n",
       "      <td>161</td>\n",
       "      <td>airplanes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tnt apologises airing 'castle' bomb episode d...</td>\n",
       "      <td>tv cable network , explosive bombs</td>\n",
       "      <td>tnt</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>katic trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mike judge beavis butthead voices google docs...</td>\n",
       "      <td>provided voices beavis butthead</td>\n",
       "      <td>mike judge</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>mike judge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>aegisthus greek mythology greek mythology , a...</td>\n",
       "      <td>son agamemnon , avenged father 's death killi...</td>\n",
       "      <td>orestes</td>\n",
       "      <td>187</td>\n",
       "      <td>194</td>\n",
       "      <td>orestes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>san antonio texas history timeline history st...</td>\n",
       "      <td>1718 texas town founded martin de alarcon fat...</td>\n",
       "      <td>san antonio</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>san antonio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentences  \\\n",
       "0    free flashcards authors studystack spain 1959...   \n",
       "1    california facts , map state symbols enchante...   \n",
       "2    price convenience? atm surcharge debate jul 1...   \n",
       "3    steamboat willy classic cartoons pinterest st...   \n",
       "4    eastern europe see section 2 2 6 nation state...   \n",
       "5    us state longest shoreline?? guide humans ala...   \n",
       "6    north dakota pumps 1 million barrels oil day ...   \n",
       "7    day day npr hear day day program march 20 , 2...   \n",
       "8    daffy known voice mel blanc 1937 1989 daffy d...   \n",
       "9    region 4 russ nelson 's home page glossary ba...   \n",
       "10   twinkle chubbins astonishing adventures natur...   \n",
       "11   november 14 , 2008 eagerly crave rhymes wave ...   \n",
       "12   cia headquarters named president bush cia hea...   \n",
       "13   puss boots shrek wikipedia puss boots fiction...   \n",
       "14   benet , william rose sgeresultat aalborg bibl...   \n",
       "15   life begins 40 boeing blogs aug 17 , 2007 thi...   \n",
       "16   tnt apologises airing 'castle' bomb episode d...   \n",
       "17   mike judge beavis butthead voices google docs...   \n",
       "18   aegisthus greek mythology greek mythology , a...   \n",
       "19   san antonio texas history timeline history st...   \n",
       "\n",
       "                                             question                 answer  \\\n",
       "0    spain 1959 , wrote dangerous summer , story r...              hemingway   \n",
       "1    valley 282 feet sea level state lowest point ...             california   \n",
       "2    like banks , many grocery stores dispensing c...                   atms   \n",
       "3                voice mickey mouse steamboat willie             walt disney   \n",
       "4          eastern european capital city 2 2 million               bucharest   \n",
       "5      6 , 640 miles coast , state longest shoreline                  alaska   \n",
       "6          pumps one million barrels oil day , state                   texas   \n",
       "7    day day things considered among programs goin...                    npr   \n",
       "8                    voice daffy duck first 50 years               mel blanc   \n",
       "9           braced framework carrying railroad chasm                 trestle   \n",
       "10   name laura bancroft , wrote twinkle chubbins ...           l frank baum   \n",
       "11   considered healthiest state 2006 , 's also ho...              minnesota   \n",
       "12   headquarters compound langley , virginia name...                    cia   \n",
       "13                         voiced puss boots shrek 2        antonio banderas   \n",
       "14   william rose benet pulitzer dust god , brothe...  stephen vincent benet   \n",
       "15   boeing manufacturing plant everett world 's l...             washington   \n",
       "16                tv cable network , explosive bombs                     tnt   \n",
       "17                   provided voices beavis butthead              mike judge   \n",
       "18   son agamemnon , avenged father 's death killi...                orestes   \n",
       "19   1718 texas town founded martin de alarcon fat...            san antonio   \n",
       "\n",
       "    start  end                predict  \n",
       "0     101  110              hemingway  \n",
       "1       1   11             california  \n",
       "2     127  131                   atms  \n",
       "3     135  146            walt disney  \n",
       "4     231  240              bucharest  \n",
       "5      43   49                 alaska  \n",
       "6     167  172           north dakota  \n",
       "7       9   12                    npr  \n",
       "8      19   28              mel blanc  \n",
       "9      93  100            russ nelson  \n",
       "10    198  210           l frank baum  \n",
       "11    128  137              minnesota  \n",
       "12      1    4                 center  \n",
       "13    222  238                antonio  \n",
       "14    117  138  stephen vincent benet  \n",
       "15    151  161              airplanes  \n",
       "16      1    4             katic trip  \n",
       "17      1   11             mike judge  \n",
       "18    187  194                orestes  \n",
       "19      1   12            san antonio  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = get_output_post_fn(valid_data, sub_output)\n",
    "valid_data['predict'] = sub\n",
    "valid_data.pop('sub')\n",
    "valid_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84219a4e-a5ce-475c-a05f-35b1b426d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(X, Y):\n",
    "    X_, Y_ = [], []\n",
    "    \n",
    "    X_ = nltk_token_string(X)\n",
    "    Y_ = nltk_token_string(Y)\n",
    "\n",
    "    m = len(X_)\n",
    "    n = len(Y_)\n",
    " \n",
    "    # declaring the array for storing the dp values\n",
    "    L = [[None]*(n + 1) for i in range(m + 1)]\n",
    " \n",
    "    \"\"\"Following steps build L[m + 1][n + 1] in bottom up fashion\n",
    "    Note: L[i][j] contains length of LCS of X[0..i-1]\n",
    "    and Y[0..j-1]\"\"\"\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0 :\n",
    "                L[i][j] = 0\n",
    "            elif X_[i-1] == Y_[j-1]:\n",
    "                L[i][j] = L[i-1][j-1]+1\n",
    "            else:\n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    " \n",
    "    # L[m][n] contains the length of LCS of X[0..n-1] & Y[0..m-1]\n",
    "    return L[m][n]\n",
    "\n",
    "\n",
    "def acc(full, sub):\n",
    "    common = lcs(full, sub)\n",
    "    union = len(full) + len(sub) - common\n",
    "    accuracy = float(common/union)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7942d36b-6c35-4832-b72e-e2273f7f4b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joeyliang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7966314514917239\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "acc_sum = 0\n",
    "for i in range(valid_data.shape[0]):\n",
    "    accuracy = acc(valid_data.iloc[i][\"answer\"], valid_data.iloc[i]['sub'])\n",
    "    acc_sum += accuracy\n",
    "\n",
    "print(\"accuracy: \", acc_sum/valid_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b718798-a133-4431-a2a4-a8667e57bfd6",
   "metadata": {},
   "source": [
    "# output for submition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b06da9bd-b401-4992-b554-eadbe7e71f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1703/1703 [01:16<00:00, 22.35it/s]\n"
     ]
    }
   ],
   "source": [
    "test_sub_output, test_predict_pos = predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "848347fc-3825-46b7-9efa-d3f7857b4527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27228</th>\n",
       "      <td>imperilment ! feb 25 , 2015 1000 guinness say...</td>\n",
       "      <td>guinness says number users language , devised...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>james</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27229</th>\n",
       "      <td>shortz ktep relieve duties letter make believ...</td>\n",
       "      <td>letter added amiable gets word means thing</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27230</th>\n",
       "      <td>visual codes secrecy photography death resear...</td>\n",
       "      <td>tongan word something limits also national ge...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unusual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27231</th>\n",
       "      <td>coincaesarioncoin windows 2016813 ptolemy xv ...</td>\n",
       "      <td>egyptian ruler ptolemy xv aka caesarion</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27232</th>\n",
       "      <td>kosher search id com pack , 's certified l'ch...</td>\n",
       "      <td>wo n't find orthodox union trademark lobsters...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>kosher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27233</th>\n",
       "      <td>midnight cowboy 1969 best picture fikkle fame...</td>\n",
       "      <td>1969 buddy pic first x rated film win best pi...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>horses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27234</th>\n",
       "      <td>bees , wasps , hornets today found apr 14 , 2...</td>\n",
       "      <td>big difference bees close relatives bees feed...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bumblebee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27235</th>\n",
       "      <td>song 44 sappho revisited 'oral' text song aug...</td>\n",
       "      <td>trojan hero hector share syllable</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27236</th>\n",
       "      <td>day 3 panmunjom , say americans weecheng com ...</td>\n",
       "      <td>army bases area known 3 letter term panmunjom...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>north korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27237</th>\n",
       "      <td>kramer vs kramer 1979 best picture fikkle fam...</td>\n",
       "      <td>gore vidal removed name salacious 1979 film m...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>original screenplay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27238</th>\n",
       "      <td>hummingbird search id bees small nectar sippi...</td>\n",
       "      <td>giant type bird imposing 8 inches long weighs...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hummingbird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27239</th>\n",
       "      <td>joan queen castile aragon britannica com joan...</td>\n",
       "      <td>spain 's queen joan mad catherine aragon</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>castile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27240</th>\n",
       "      <td>cussed define cussed dictionary com 1765 75 ,...</td>\n",
       "      <td>variant curse also mean troublesome animal</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cussed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27241</th>\n",
       "      <td>national woman suffrage association revolvy n...</td>\n",
       "      <td>unable vote , 1869 , susan b anthony formed n...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>elizabeth cady stanton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242</th>\n",
       "      <td>full text gleanings bee culture internet arch...</td>\n",
       "      <td>'s ladylike ! 2 bees emerge cells time , figh...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27243</th>\n",
       "      <td>persephone greek goddess underworld persephon...</td>\n",
       "      <td>greek goddess persephone</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>underworld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27244</th>\n",
       "      <td>fil 2032 history film ii malcolm armstrong co...</td>\n",
       "      <td>private eye mickey rourke works louis cyphre ...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>private detective's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27245</th>\n",
       "      <td>royal jellies memidex dictionary thesaurus se...</td>\n",
       "      <td>food bee larvae called royal 's secreted head...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27246</th>\n",
       "      <td>charles ix definition charles ix free diction...</td>\n",
       "      <td>french kings francis ii , charles ix henry iii</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>france</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27247</th>\n",
       "      <td>bob 's blog page 41 dec 16 , 2010 category sk...</td>\n",
       "      <td>construction boom fueled oil gas money , capi...</td>\n",
       "      <td>answer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>moscow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  \\\n",
       "27228   imperilment ! feb 25 , 2015 1000 guinness say...   \n",
       "27229   shortz ktep relieve duties letter make believ...   \n",
       "27230   visual codes secrecy photography death resear...   \n",
       "27231   coincaesarioncoin windows 2016813 ptolemy xv ...   \n",
       "27232   kosher search id com pack , 's certified l'ch...   \n",
       "27233   midnight cowboy 1969 best picture fikkle fame...   \n",
       "27234   bees , wasps , hornets today found apr 14 , 2...   \n",
       "27235   song 44 sappho revisited 'oral' text song aug...   \n",
       "27236   day 3 panmunjom , say americans weecheng com ...   \n",
       "27237   kramer vs kramer 1979 best picture fikkle fam...   \n",
       "27238   hummingbird search id bees small nectar sippi...   \n",
       "27239   joan queen castile aragon britannica com joan...   \n",
       "27240   cussed define cussed dictionary com 1765 75 ,...   \n",
       "27241   national woman suffrage association revolvy n...   \n",
       "27242   full text gleanings bee culture internet arch...   \n",
       "27243   persephone greek goddess underworld persephon...   \n",
       "27244   fil 2032 history film ii malcolm armstrong co...   \n",
       "27245   royal jellies memidex dictionary thesaurus se...   \n",
       "27246   charles ix definition charles ix free diction...   \n",
       "27247   bob 's blog page 41 dec 16 , 2010 category sk...   \n",
       "\n",
       "                                                question  answer  start  end  \\\n",
       "27228   guinness says number users language , devised...  answer      0    0   \n",
       "27229        letter added amiable gets word means thing   answer      0    0   \n",
       "27230   tongan word something limits also national ge...  answer      0    0   \n",
       "27231           egyptian ruler ptolemy xv aka caesarion   answer      0    0   \n",
       "27232   wo n't find orthodox union trademark lobsters...  answer      0    0   \n",
       "27233   1969 buddy pic first x rated film win best pi...  answer      0    0   \n",
       "27234   big difference bees close relatives bees feed...  answer      0    0   \n",
       "27235                 trojan hero hector share syllable   answer      0    0   \n",
       "27236   army bases area known 3 letter term panmunjom...  answer      0    0   \n",
       "27237   gore vidal removed name salacious 1979 film m...  answer      0    0   \n",
       "27238   giant type bird imposing 8 inches long weighs...  answer      0    0   \n",
       "27239          spain 's queen joan mad catherine aragon   answer      0    0   \n",
       "27240        variant curse also mean troublesome animal   answer      0    0   \n",
       "27241   unable vote , 1869 , susan b anthony formed n...  answer      0    0   \n",
       "27242   's ladylike ! 2 bees emerge cells time , figh...  answer      0    0   \n",
       "27243                          greek goddess persephone   answer      0    0   \n",
       "27244   private eye mickey rourke works louis cyphre ...  answer      0    0   \n",
       "27245   food bee larvae called royal 's secreted head...  answer      0    0   \n",
       "27246    french kings francis ii , charles ix henry iii   answer      0    0   \n",
       "27247   construction boom fueled oil gas money , capi...  answer      0    0   \n",
       "\n",
       "                      predict  \n",
       "27228                   james  \n",
       "27229                 believe  \n",
       "27230                 unusual  \n",
       "27231                          \n",
       "27232                  kosher  \n",
       "27233                  horses  \n",
       "27234               bumblebee  \n",
       "27235                   greek  \n",
       "27236            north korean  \n",
       "27237     original screenplay  \n",
       "27238             hummingbird  \n",
       "27239                 castile  \n",
       "27240                  cussed  \n",
       "27241  elizabeth cady stanton  \n",
       "27242                     bee  \n",
       "27243              underworld  \n",
       "27244     private detective's  \n",
       "27245                   jelly  \n",
       "27246                  france  \n",
       "27247                  moscow  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub = get_output_post_fn(test_data, test_sub_output)\n",
    "test_data['predict'] = test_sub\n",
    "test_data.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b51f752b-15f5-4a2f-b02f-4f76af36b8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27248"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7794167-420f-49f9-9ada-66fd9b503387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27248it [00:00, 65252.59it/s]\n"
     ]
    }
   ],
   "source": [
    "test_submit_data  = read_data_from_txt(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ff27ffbd-ee5a-47e9-a094-91f64fd40830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" 's largest kingdom united kingdom \""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_submit_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c95096e1-f4f6-404e-89c5-25ac1a79f410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27248\n",
      "27248\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "question = []\n",
    "for i,j,k in test_submit_data:\n",
    "    text.append(i)\n",
    "    question.append(j)\n",
    "print(len(text))\n",
    "print(len(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9ad9b39-dd3d-4817-acc3-192eca61e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/final-submit.txt'\n",
    "f = open(path, 'w')\n",
    "for question, answer in zip(question, test_sub):\n",
    "    f.write(question+\"|||\"+answer+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec43a6-9306-4162-9507-34b8eb303de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

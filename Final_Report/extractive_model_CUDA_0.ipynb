{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5468be7-7b35-49c7-a24b-c66e0bc97fb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6635d25c-bcc5-43be-aa8f-2a74c7b82542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 12:34:06.237024: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-03 12:34:06.429669: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-03 12:34:07.023381: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:\n",
      "2023-01-03 12:34:07.023451: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:\n",
      "2023-01-03 12:34:07.023461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from fastbm25 import fastbm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a259fc38-229b-4551-986d-32f540d619d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea3294a-50d2-4b74-9375-5b051a107a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/QA_model_TFIDF_k_4\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_type_BM25(T, k):\n",
    "    if T:\n",
    "        K = k\n",
    "        MODEL_PATH = 'model/QA_modelBM25_k_'+str(K)\n",
    "    else:\n",
    "        K = k\n",
    "        MODEL_PATH = 'model/QA_model_TFIDF_k_'+str(K)\n",
    "    print(MODEL_PATH)\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "    return MODEL_PATH, K, device\n",
    "\n",
    "MODEL_PATH, K, device = evaluate_type_BM25(False, 4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b6da2-bac0-4eff-8ce9-b221cfceedc6",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774bdead-0f1a-4def-bf6a-972b61e141d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/train.txt'\n",
    "DEV_PATH = 'data/val.txt'\n",
    "TEST_PATH = 'data/test.txt'\n",
    "TEST_ANSWER_PATH = 'data/Assignment2_test_answer.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd77046-b89c-45e4-b46a-86ef202bb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_txt(path):\n",
    "    QandA = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file):\n",
    "            #print(line)\n",
    "            if line != \"\\n\":\n",
    "                splitted = line.split(\"|||\")\n",
    "                sentences = splitted[0]\n",
    "                question  = r\" \".join(splitted[1].split())\n",
    "                answer    = re.sub(\"\\n\",\"\",splitted[2])\n",
    "                answer = r\" \".join(answer.split())\n",
    "                QandA.append((sentences, question, answer))\n",
    "    return QandA\n",
    "\n",
    "def read_answer_data(path):\n",
    "    QandA = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file):\n",
    "            #print(line)\n",
    "            if line != \"\\n\":\n",
    "                splitted = line.split(\"|||\")\n",
    "                question = r\" \".join(splitted[0].split())\n",
    "                answer  = re.sub(\"\\n\",\"\",splitted[-1])\n",
    "                answer = r\" \".join(answer.split())\n",
    "                QandA.append((question, answer))\n",
    "    return QandA\n",
    "\n",
    "def correct_test_answer(data, answer):\n",
    "    QandA = []\n",
    "    for origin, correct in zip(data, answer):\n",
    "        sentence = origin[0]\n",
    "        question = origin[1]\n",
    "        answer   = correct[1]\n",
    "        QandA.append((sentence, question, answer))\n",
    "    return  QandA   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a560c18e-58af-471f-9e7a-75bd1f2bec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99820it [00:01, 64057.45it/s]\n",
      "13893it [00:00, 58748.33it/s]\n",
      "27248it [00:00, 61842.65it/s]\n",
      "27248it [00:00, 232663.48it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data  = read_data_from_txt(TRAIN_PATH)\n",
    "del train_data[51641] #51641報錯\n",
    "valid_data  = read_data_from_txt(DEV_PATH)\n",
    "# test_data\n",
    "test_data   = read_data_from_txt(TEST_PATH)\n",
    "test_answer = read_answer_data(TEST_ANSWER_PATH)\n",
    "test_data   = correct_test_answer(test_data, test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346dec2b-0063-44f5-ac91-042237167740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99819\n",
      "13893\n",
      "27248\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(valid_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c8eb4-5cb1-4ab4-90fe-b322e536505c",
   "metadata": {},
   "source": [
    "# 用tf-idf 選取和問題最相近的句子，包含答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d428cfe-086d-41f7-85c3-3347058aa96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_top_k_articles(query, docs, k=1):\\n\\n    # Initialize a vectorizer that removes English stop words\\n    vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words=\\'english\\')\\n\\n    # Create a corpus of query and documents and convert to TFIDF vectors\\n    query_and_docs = [query] + docs\\n    matrix = vectorizer.fit_transform(query_and_docs)\\n\\n    # Holds our cosine similarity scores\\n    scores = []\\n\\n    # The first vector is our query text, so compute the similarity of our query against all document vectors\\n    for i in range(1, len(query_and_docs)):\\n        scores.append(cosine_similarity(matrix[0], matrix[i])[0][0])\\n\\n    # Sort list of scores and return the top k highest scoring documents\\n    sorted_list = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\\n    top_doc_indices = [x[0] for x in sorted_list[:k]]\\n    top_docs = [docs[x] for x in top_doc_indices]\\n  \\n    return top_docs\\n\\ndef get_topk_tfidf_sentence(data):\\n    contexts = []\\n    for sentences, question, answer in tqdm(data):\\n        contained_answer_sentence = []\\n        sentences = re.findall(r\\'<s>(.*?)</s>\\', sentences)\\n        for sentence in sentences:\\n            if answer in sentence:\\n                contained_answer_sentence.append(sentence)\\n        doc = get_top_k_articles(question, contained_answer_sentence)\\n        context = \". \".join(doc)\\n        contexts.append(context)\\n    return contexts\\ntrain_contexts = get_topk_tfidf_sentence(train_data)\\nvalid_contexts =   get_topk_tfidf_sentence(valid_data)\\ntest_contexts  = get_topk_tfidf_sentence(test_data)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_top_k_articles(query, docs, k=1):\n",
    "\n",
    "    # Initialize a vectorizer that removes English stop words\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words='english')\n",
    "\n",
    "    # Create a corpus of query and documents and convert to TFIDF vectors\n",
    "    query_and_docs = [query] + docs\n",
    "    matrix = vectorizer.fit_transform(query_and_docs)\n",
    "\n",
    "    # Holds our cosine similarity scores\n",
    "    scores = []\n",
    "\n",
    "    # The first vector is our query text, so compute the similarity of our query against all document vectors\n",
    "    for i in range(1, len(query_and_docs)):\n",
    "        scores.append(cosine_similarity(matrix[0], matrix[i])[0][0])\n",
    "\n",
    "    # Sort list of scores and return the top k highest scoring documents\n",
    "    sorted_list = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    top_doc_indices = [x[0] for x in sorted_list[:k]]\n",
    "    top_docs = [docs[x] for x in top_doc_indices]\n",
    "  \n",
    "    return top_docs\n",
    "\n",
    "def get_topk_tfidf_sentence(data):\n",
    "    contexts = []\n",
    "    for sentences, question, answer in tqdm(data):\n",
    "        contained_answer_sentence = []\n",
    "        sentences = re.findall(r'<s>(.*?)</s>', sentences)\n",
    "        for sentence in sentences:\n",
    "            if answer in sentence:\n",
    "                contained_answer_sentence.append(sentence)\n",
    "        doc = get_top_k_articles(question, contained_answer_sentence)\n",
    "        context = \". \".join(doc)\n",
    "        contexts.append(context)\n",
    "    return contexts\n",
    "train_contexts = get_topk_tfidf_sentence(train_data)\n",
    "valid_contexts =   get_topk_tfidf_sentence(valid_data)\n",
    "test_contexts  = get_topk_tfidf_sentence(test_data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54cf77cd-d710-4650-b7a4-a78f56b2511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 99819/99819 [02:22<00:00, 698.84it/s]\n",
      "100%|████████████████████████████████████| 13893/13893 [00:20<00:00, 675.79it/s]\n",
      "100%|████████████████████████████████████| 27248/27248 [00:32<00:00, 832.28it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_topk_bm25_sentence(data, k):\n",
    "    contexts = []\n",
    "    for sentences, question, answer in tqdm(data):\n",
    "        contained_answer_sentence = []\n",
    "        sentences = re.findall(r'<s>(.*?)</s>', sentences)\n",
    "        for sentence in sentences:\n",
    "            if answer in sentence:\n",
    "                contained_answer_sentence.append(sentence)\n",
    "        tokenized_corpus = [doc.lower().split(\" \") for doc in contained_answer_sentence]\n",
    "        FASTBM25 = fastbm25(tokenized_corpus)\n",
    "        tokenized_answer = question.lower().split(\" \")\n",
    "        doc = FASTBM25.top_k_sentence(tokenized_answer, k=k)\n",
    "        final_doc = []\n",
    "        for list_doc, _,score in doc:\n",
    "            final_doc.append(\" \".join(list_doc))\n",
    "        context = \". \".join(final_doc)\n",
    "        contexts.append(context)\n",
    "    return contexts\n",
    "\n",
    "train_contexts = get_topk_bm25_sentence(train_data, K)\n",
    "valid_contexts = get_topk_bm25_sentence(valid_data, K)\n",
    "test_contexts  = get_topk_bm25_sentence(test_data,  K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45e935-3121-4efc-9239-2a6d8ddeae7b",
   "metadata": {},
   "source": [
    "# 轉換成Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1b22824-930b-4364-b439-1d6949bf00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(data, contexts):\n",
    "    ques = []\n",
    "    ans= []\n",
    "    for sentences, question, answer in data:\n",
    "        ques.append(question)\n",
    "        ans.append(answer)\n",
    "    return pd.DataFrame({'sentences':contexts,'question':ques,'answer':ans})\n",
    "train_data = convert_to_dataframe(train_data, train_contexts)\n",
    "valid_data = convert_to_dataframe(valid_data, valid_contexts)\n",
    "test_data =  convert_to_dataframe(test_data, test_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0536454f-cb60-4bfe-b32c-78bbb9c9b7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82113</th>\n",
       "      <td>walking big apple june 2009 jun 1 , 2009 recent productions continue add individual collective marx brothers broadway , notes new york theatres 1920s brothers , especially arthur born adolph arthur marx brothers leonard chico , julius groucho , milton gummo , herbert .  marx brothers harpo marx , groucho marx , chico marx set young marx brothers arthur harpo , milton gummo , leonard chico julius groucho marx brothers excessively large scissors marx brothers arthur harpo , julius groucho , herbert zeppo leonard chico one famous comedy stooges history stage screen .  beyond photography marx brothers , meet magnum photos jul 24 , 2012 marx brothers chico , harpo , groucho zeppo 1931 image chances , mother called leonard , arthur , julius herbert ever graced silver screen , experience shows 's magnum name endures , 's also 's made rare .  brother , photographs pictures pinterest last picture 5 marx brothers last photograph five brothers together face name photographs strangers outdoor wedding mauna lani bay hotel big island hawaii wo n't recognize arthur , herbert , milton , leonard julius unless screen shot pm png</td>\n",
       "      <td>collective big screen name brothers arthur , herbert , julius leonard</td>\n",
       "      <td>marx brothers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_random_elements(dataset, num_examples=1):\n",
    "    df = dataset.sample(n = num_examples)\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "show_random_elements(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4868ee98-14ba-4a60-a812-1888c457d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99741\n",
      "13885\n",
      "27217\n"
     ]
    }
   ],
   "source": [
    "def clean_answer_not_in_context(data):\n",
    "    index_ids=[]\n",
    "    for i in range(len(data)):\n",
    "        if data['answer'].iloc[i] in data['sentences'].iloc[i]:\n",
    "            continue\n",
    "        else:\n",
    "            index_ids.append(i)\n",
    "    data = data.drop(index_ids, axis=0)\n",
    "    print(len(data))\n",
    "    return data\n",
    "train_data = clean_answer_not_in_context(train_data)\n",
    "valid_data = clean_answer_not_in_context(valid_data)\n",
    "test_data  = clean_answer_not_in_context(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1445d-7837-4526-a51e-a1b46f8d37a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d5702f7-78e5-4352-8b60-e6237e434409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "999321da-7300-4195-b283-62521c2550eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokennize_list(data):\n",
    "    data_question = data['question'].tolist()\n",
    "    data_context = data['sentences'].tolist()\n",
    "    return data_question, data_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df9f390-6399-4f0b-8df8-276a12fe6da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_question, train_context = get_tokennize_list(train_data)\n",
    "valid_question, valid_context = get_tokennize_list(valid_data)\n",
    "test_question,  test_context  = get_tokennize_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad7dd308-7f92-46e7-a019-5a22fa1c1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_question, train_context, truncation=True, padding=True)\n",
    "val_encodings   = tokenizer(valid_question, valid_context, truncation=True, padding=True)\n",
    "test_encodings  = tokenizer(test_question, test_context, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00bd02bf-c0f6-406c-a71c-715ecc6d05ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] last 8 years life, galileo house arrest espousing man's theory [SEP] lebowski sweater replica jun 17, 2013 history last 8 years life, galileo house arrest espousing man's theory copernicus espn's top 10 time. images knickerless women jun 17, 2013 history last 8 years life, galileo house arrest espousing man's theory copernicus espn's top 10 time. christa mcauliffe autopsy jun 17, 2013 history last 8 years life, galileo house arrest espousing man's theory copernicus espn's top 10 time. mary nightingale botox jun 17, 2013 history last 8 years life, galileo house arrest espousing man's theory copernicus espn's top 10 time [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8ab73-cda1-4f54-876a-8a03be778328",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 找答案的start end index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf0d315-ac97-4b61-905a-1401f3d6a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Start_End_index(data):\n",
    "    data['start'] = [y.index(x) for x,y in zip(data[\"answer\"],data[\"sentences\"])]\n",
    "    data['end']   = [x+len(str(y)) for x,y in zip(data[\"start\"],data[\"answer\"])]\n",
    "get_Start_End_index(train_data)\n",
    "get_Start_End_index(valid_data)\n",
    "get_Start_End_index(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eeadd48-fc3e-48ff-94fa-feee17840650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lebowski sweater replica jun 17 , 2013 histor...</td>\n",
       "      <td>last 8 years life , galileo house arrest espou...</td>\n",
       "      <td>copernicus</td>\n",
       "      <td>113</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jim thorpe bio , stats , results olympics spo...</td>\n",
       "      <td>2 1912 olympian football star carlisle indian ...</td>\n",
       "      <td>jim thorpe</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yuma arizona oddities part 3 jul 21 , 2011 yu...</td>\n",
       "      <td>city yuma state record average 4 , 055 hours s...</td>\n",
       "      <td>arizona</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gravestone famous ancestor roger sherman sign...</td>\n",
       "      <td>signer dec indep , framer constitution mass , ...</td>\n",
       "      <td>john adams</td>\n",
       "      <td>176</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full text catalog educational films internet ...</td>\n",
       "      <td>title aesop fable , insect shared billing gras...</td>\n",
       "      <td>ant</td>\n",
       "      <td>251</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0   lebowski sweater replica jun 17 , 2013 histor...   \n",
       "1   jim thorpe bio , stats , results olympics spo...   \n",
       "2   yuma arizona oddities part 3 jul 21 , 2011 yu...   \n",
       "3   gravestone famous ancestor roger sherman sign...   \n",
       "4   full text catalog educational films internet ...   \n",
       "\n",
       "                                            question      answer  start  end  \n",
       "0  last 8 years life , galileo house arrest espou...  copernicus    113  123  \n",
       "1  2 1912 olympian football star carlisle indian ...  jim thorpe      1   11  \n",
       "2  city yuma state record average 4 , 055 hours s...     arizona      6   13  \n",
       "3  signer dec indep , framer constitution mass , ...  john adams    176  186  \n",
       "4  title aesop fable , insect shared billing gras...         ant    251  254  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "724fd848-c32e-444b-9f0b-c52dece503f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'copernicus'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['sentences'].iloc[0][113:123]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497799e4-ac87-4d75-b9ef-58e34bafc684",
   "metadata": {
    "tags": []
   },
   "source": [
    "# add token positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1c9cf6e-3f34-4cac-a7fb-4c12eb74ca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answer = train_data[['start', 'end']].to_dict('records')\n",
    "valid_answer = valid_data[['start', 'end']].to_dict('records')\n",
    "test_answer  = test_data[['start', 'end']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cd79af5-0bf7-4b8a-8da7-f2070c2d3835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copernicus\n",
      "47\n",
      "49\n",
      "copernicus\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(train_data['answer'].iloc[i])\n",
    "a = train_encodings.char_to_token(i, train_answer[i]['start'], 1)\n",
    "print(a)\n",
    "b = train_encodings.char_to_token(i, train_answer[i]['end']-1, 1)\n",
    "print(b)\n",
    "print(tokenizer.decode(train_encodings['input_ids'][i][47:b+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24954ba0-9a6f-420c-8968-875574a11d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['start'],1))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['end']-1,1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "            \n",
    "        shift = 1\n",
    "        while end_positions[-1] is None:\n",
    "            if answers[i]['end'] - shift>=0:\n",
    "                end_positions[-1] = encodings.char_to_token(i, answers[i]['end'] - shift, 1)\n",
    "                shift += 1 \n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    encodings.update({'start': start_positions, 'end': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answer)\n",
    "add_token_positions(val_encodings, valid_answer)\n",
    "add_token_positions(test_encodings, test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bf1d76e-2865-4a5f-9602-ec4e2b2f50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert char_based_id to token_based_id\n",
    "# Find the corossponding token id after input being tokenized\n",
    "add_token_positions(train_encodings, train_answer)\n",
    "add_token_positions(val_encodings, valid_answer)\n",
    "add_token_positions(test_encodings, test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76f41d22-155f-4ccf-8748-6c5082b6aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start', 'end'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4db46f0a-2c1f-4a6c-a116-079bcde29c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "49\n",
      "36\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['start'][0])\n",
    "print(train_encodings['end'][0])\n",
    "print(val_encodings['start'][0])\n",
    "print(val_encodings['end'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c945d874-6b91-4713-862d-97c0d7efb559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oratory\n",
      "oratory\n"
     ]
    }
   ],
   "source": [
    "i=23\n",
    "print(train_data['sentences'].iloc[i][train_answer[i]['start']:train_answer[i]['end']])\n",
    "print(tokenizer.decode(train_encodings['input_ids'][i][train_encodings['start'][i]:train_encodings['end'][i]+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381f5a8-6242-47ed-a3cb-b05c5e3407aa",
   "metadata": {},
   "source": [
    "# convert to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76557abc-b0a1-4f14-9062-1adeb15ba9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            return {key: torch.tensor(value[idx]) for key, value in self.encodings.items()}\n",
    "        except:\n",
    "            print(idx)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1f3a9ae-0f63-418c-b390-ee1197c57d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QADataset(train_encodings)\n",
    "val_dataset   = QADataset(val_encodings)\n",
    "test_dataset  = QADataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5d2e738-5daa-4a8f-af49-1ded38a49528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 22620,  1394,  3003,   117,  1724,  4249,  2247,   117,  1642,\n",
       "          6048, 12200, 22217,  1116,   102,  1714,  6746, 18464,  5752,  2025,\n",
       "          8419,  2158, 22620,  1394,  3003,   117,  1724,  4249,  2247,   117,\n",
       "          1642,  6048, 12200, 22217,  1116,   117, 23123,  1158,  2787,  6394,\n",
       "           119,  1185,  2707, 10615,  1489,   117,  1369,  1270,  7761,  1821,\n",
       "         26237,  1389,  5752, 22620,  1394,  3003,   117,  1724,  4249,  2247,\n",
       "           117,  1642,  6048, 12200, 22217,  1116, 23123,  1158,  2787,  6394,\n",
       "           119,  1299,  1186,  1299,  1186,  6048,  3377, 22620,  1394,  1821,\n",
       "         26237,  1161, 23123,  1158,  2787,  5016, 23123,  1158,  2787,   117,\n",
       "           175, 18318, 25300,  1202,  5031, 14846,  8037,  7959,  5075,   117,\n",
       "          3494,  2766,  2286,  3003,  1346,  2761,   117,  4249,  2247,  1338,\n",
       "         12890, 19587,  1676, 12477,  1616,   117,  1724,  1893,  1603,  1642,\n",
       "           117,  1141,  9616, 12200, 22217,  1116, 20856,  2227,  9521,  1226,\n",
       "          1299,  6738, 15625,   119, 14940,  3761,  4249,  2247, 14044,  3965,\n",
       "          1204, 23123,  1158,  2787,  4156,  1297,  2435,  3003,  3593,  2208,\n",
       "          1632,  8492,  2944, 12200, 22217,  1116,  1344,  1278,   117, 23123,\n",
       "          1158,  2787,  4927,  1684,  8598,  6409,  1162,   117,  1278,  3054,\n",
       "           117,  1724,  1148,  1558,  2281,   117,  3336,  1145,  9440,   117,\n",
       "          1642,  1821, 26237,  1389,  4035, 23655,  2737,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'start': tensor(36),\n",
       " 'end': tensor(38)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674f8be-152b-4b8c-9377-fc6491c55358",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7b0ddb-6819-4ae5-9446-d18b50cea7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class QAModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
    "        logits = output[0]\n",
    "        out = self.fc(logits)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0acf7597-0e1d-4e17-b6b1-a7640efaa198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "model = QAModel()#.to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40047f8d-dac5-4902-b39f-a98838431248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 413.185MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084bcc6b-77e5-46a7-a96a-01f320c4dea1",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d7b5768-3a39-4a3e-a260-880fd894ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack data into dataloader by batch\n",
    "batch_size   = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1515b417-4606-44a5-a2b7-606e9f4a380b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12468\n",
      "1736\n",
      "3403\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(valid_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6bd4552-cd8f-4b3d-a25d-5a4345199aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epoch = 3\n",
    "loss_fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcefb50d-c7fb-4c88-af70-b298500bae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(valid_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(valid_loader, leave=True)\n",
    "        for batch_id, batch in enumerate(loop):\n",
    "            input_ids      = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start          = batch['start'].to(device)\n",
    "            end            = batch['end'].to(device)\n",
    "\n",
    "            # model output\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "            start_logits, end_logits = torch.split(outputs, 1, 2)\n",
    "\n",
    "            start_logits = start_logits.squeeze(-1).contiguous()\n",
    "            end_logits   = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "            start_loss = loss_fct(start_logits, start)\n",
    "            end_loss   = loss_fct(end_logits, end)\n",
    "\n",
    "            loss = start_loss + end_loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print('Validation Loss {:.4f}'.format(running_loss / len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c3418-4101-4e76-84c7-8c8fae8f5b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████| 12468/12468 [2:06:24<00:00,  1.64it/s, loss=4.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss 1.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1736/1736 [05:00<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss 1.3802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████| 12468/12468 [2:06:46<00:00,  1.64it/s, loss=12.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss 7.2441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1736/1736 [04:59<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss 12.2006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  10%|█▌              | 1208/12468 [12:15<1:53:54,  1.65it/s, loss=12.5]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 2:  50%|███████       | 6271/12468 [1:03:44<1:02:41,  1.65it/s, loss=12.5]"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch_id, batch in enumerate(loop):\n",
    "        # reset\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids      = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        start          = batch['start'].to(device)\n",
    "        end            = batch['end'].to(device)\n",
    "\n",
    "        # model output\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        start_logits, end_logits = torch.split(outputs, 1, 2)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits   = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        start_loss = loss_fct(start_logits, start)\n",
    "        end_loss = loss_fct(end_logits, end)\n",
    "        loss = start_loss + end_loss\n",
    "        # calculate loss\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    print('Training Loss {:.4f}'.format(running_loss / len(train_loader)))\n",
    "    evaluate(valid_loader)\n",
    "    \n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "559178d0-0f62-40c2-8f48-56973ee9994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd3275-7a0f-4de9-bf5c-bc01ccd55a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb31815-0108-4a1c-9883-7b21950ad954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predict_pos = []\n",
    "    sub_output = []\n",
    "\n",
    "    loop = tqdm(test_loader, leave=True)\n",
    "    for batch_id, batch in enumerate(loop):\n",
    "        input_ids      = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "        # model output\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        start_logits, end_logits = torch.split(outputs, 1, 2)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits   = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        start_prdict = torch.argmax(start_logits, 1).cpu().numpy()\n",
    "        end_prdict   = torch.argmax(end_logits, 1).cpu().numpy()\n",
    "\n",
    "        for i in range(len(input_ids)):\n",
    "            predict_pos.append((start_prdict[i].item(), end_prdict[i].item()))\n",
    "            sub = tokenizer.decode(input_ids[i][start_prdict[i]:end_prdict[i]+1])\n",
    "            sub_output.append(sub)\n",
    "    \n",
    "    return sub_output, predict_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30265c-ebe2-444f-ab51-aa5fc5f73501",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_output, predict_pos = predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644686c-c4e9-4f66-865e-0f4838acdc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1604be-f788-4d92-8d6c-ada4a4329d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_token_string(sentence):\n",
    "    # print(sentence)\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        if len(tokens[i]) == 1:\n",
    "            tokens[i] = re.sub(r\"[!\\\"#$%&\\'()*\\+, -.\\/:;<=>?@\\[\\\\\\]^_`{|}~]\", '', tokens[i])\n",
    "    while '' in tokens:\n",
    "        tokens.remove('')\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18b197-10be-4b7c-afa4-bd2cbb331966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_post_fn(test, sub_output):\n",
    "    sub = []\n",
    "    for i in range(len(test)):\n",
    "\n",
    "        sub_pred = sub_output[i].split()\n",
    "\n",
    "        temp = sub_pred.copy()\n",
    "        if sub_pred is None:\n",
    "            sub_pred = []\n",
    "        else:\n",
    "            for j in range(len(temp)):\n",
    "                if temp[j] == '[SEP]':\n",
    "                    sub_pred.remove('[SEP]')\n",
    "                if temp[j] == '[PAD]':\n",
    "                    sub_pred.remove('[PAD]')\n",
    "\n",
    "        sub.append(' '.join(sub_pred))\n",
    "        \n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ff3961-2b47-4af0-a55f-5df0c03e6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = get_output_post_fn(test_data, sub_output)\n",
    "test_data['predict'] = sub\n",
    "test_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84219a4e-a5ce-475c-a05f-35b1b426d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(X, Y):\n",
    "    X_, Y_ = [], []\n",
    "    \n",
    "    X_ = nltk_token_string(X)\n",
    "    Y_ = nltk_token_string(Y)\n",
    "\n",
    "    m = len(X_)\n",
    "    n = len(Y_)\n",
    " \n",
    "    # declaring the array for storing the dp values\n",
    "    L = [[None]*(n + 1) for i in range(m + 1)]\n",
    " \n",
    "    \"\"\"Following steps build L[m + 1][n + 1] in bottom up fashion\n",
    "    Note: L[i][j] contains length of LCS of X[0..i-1]\n",
    "    and Y[0..j-1]\"\"\"\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0 :\n",
    "                L[i][j] = 0\n",
    "            elif X_[i-1] == Y_[j-1]:\n",
    "                L[i][j] = L[i-1][j-1]+1\n",
    "            else:\n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    " \n",
    "    # L[m][n] contains the length of LCS of X[0..n-1] & Y[0..m-1]\n",
    "    return L[m][n]\n",
    "\n",
    "\n",
    "def acc(full, sub):\n",
    "    common = lcs(full, sub)\n",
    "    union = len(full) + len(sub) - common\n",
    "    accuracy = float(common/union)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7942d36b-6c35-4832-b72e-e2273f7f4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "acc_sum = 0\n",
    "for i in range(valid_data.shape[0]):\n",
    "    accuracy = acc(valid_data.iloc[i][\"answer\"], valid_data.iloc[i]['predict'])\n",
    "    acc_sum += accuracy\n",
    "\n",
    "print(\"lcs accuracy: \", acc_sum/valid_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd8cb9-0b0e-4555-bbc8-30b473aaf4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import string\n",
    "  \n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  \n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "  \n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "  \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "  \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "  \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "    \n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "   \n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "  \n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def f1_metric(data):\n",
    "    f1_score = 0.0\n",
    "    for answer, pred in zip(data[\"answer\"], data['predict']):\n",
    "        f1_score+=compute_f1(answer, pred)\n",
    "    print(\"f1 score: {}\".format(f1_score/len(data)))\n",
    "f1_metric(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06da9bd-b401-4992-b554-eadbe7e71f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_output, test_predict_pos = predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848347fc-3825-46b7-9efa-d3f7857b4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub = get_output_post_fn(test_data, test_sub_output)\n",
    "test_data['predict'] = test_sub\n",
    "test_data.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f752b-15f5-4a2f-b02f-4f76af36b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7794167-420f-49f9-9ada-66fd9b503387",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_submit_data  = read_data_from_txt(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27ffbd-ee5a-47e9-a094-91f64fd40830",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_submit_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95096e1-f4f6-404e-89c5-25ac1a79f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "question = []\n",
    "for i,j,k in test_submit_data:\n",
    "    text.append(i)\n",
    "    question.append(j)\n",
    "print(len(text))\n",
    "print(len(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad9b39-dd3d-4817-acc3-192eca61e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "path = 'data/final-submit.txt'\n",
    "f = open(path, 'w')\n",
    "for question, answer in zip(question, test_sub):\n",
    "    f.write(question+\"|||\"+answer+\"\\n\")\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec43a6-9306-4162-9507-34b8eb303de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13319070-d339-4f4e-8cf4-579a2d89384a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a78ea-027c-4227-9680-5b6ba9ae20c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d0160-21a3-484b-bbea-3fb4279b2c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

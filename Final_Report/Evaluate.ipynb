{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5468be7-7b35-49c7-a24b-c66e0bc97fb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6635d25c-bcc5-43be-aa8f-2a74c7b82542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 22:07:26.300820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-02 22:07:26.492380: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-02 22:07:27.084675: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:\n",
      "2023-01-02 22:07:27.084746: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:\n",
      "2023-01-02 22:07:27.084756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from fastbm25 import fastbm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a259fc38-229b-4551-986d-32f540d619d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e02de17-1add-469e-b5f2-bf73fa3ed580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/QA_modelBM25_k_5\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_type_BM25(T, k):\n",
    "    if T:\n",
    "        K = k\n",
    "        MODEL_PATH = 'model/QA_modelBM25_k_'+str(K)\n",
    "    else:\n",
    "        K = k\n",
    "        MODEL_PATH = 'model/QA_model_TFIDF_k_'+str(K)\n",
    "    print(MODEL_PATH)\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "    return MODEL_PATH, K, device\n",
    "\n",
    "MODEL_PATH, K, device = evaluate_type_BM25(True, 5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b6da2-bac0-4eff-8ce9-b221cfceedc6",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774bdead-0f1a-4def-bf6a-972b61e141d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/train.txt'\n",
    "DEV_PATH = 'data/val.txt'\n",
    "TEST_PATH = 'data/test.txt'\n",
    "TEST_ANSWER_PATH = 'data/Assignment2_test_answer.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd77046-b89c-45e4-b46a-86ef202bb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_txt(path):\n",
    "    QandA = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file):\n",
    "            #print(line)\n",
    "            if line != \"\\n\":\n",
    "                splitted = line.split(\"|||\")\n",
    "                sentences = splitted[0]\n",
    "                question  = r\" \".join(splitted[1].split())\n",
    "                answer    = re.sub(\"\\n\",\"\",splitted[2])\n",
    "                answer = r\" \".join(answer.split())\n",
    "                QandA.append((sentences, question, answer))\n",
    "    return QandA\n",
    "\n",
    "def read_answer_data(path):\n",
    "    QandA = []\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file):\n",
    "            #print(line)\n",
    "            if line != \"\\n\":\n",
    "                splitted = line.split(\"|||\")\n",
    "                question = r\" \".join(splitted[0].split())\n",
    "                answer  = re.sub(\"\\n\",\"\",splitted[-1])\n",
    "                answer = r\" \".join(answer.split())\n",
    "                QandA.append((question, answer))\n",
    "    return QandA\n",
    "\n",
    "def correct_test_answer(data, answer):\n",
    "    QandA = []\n",
    "    for origin, correct in zip(data, answer):\n",
    "        sentence = origin[0]\n",
    "        question = origin[1]\n",
    "        answer   = correct[1]\n",
    "        QandA.append((sentence, question, answer))\n",
    "    return  QandA   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a560c18e-58af-471f-9e7a-75bd1f2bec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27248it [00:00, 61761.81it/s]\n",
      "27248it [00:00, 232305.95it/s]\n"
     ]
    }
   ],
   "source": [
    "#train_data  = read_data_from_txt(TRAIN_PATH)\n",
    "#del train_data[51641] #51641報錯\n",
    "#valid_data  = read_data_from_txt(DEV_PATH)\n",
    "# test_data\n",
    "test_data   = read_data_from_txt(TEST_PATH)\n",
    "test_answer = read_answer_data(TEST_ANSWER_PATH)\n",
    "test_data   = correct_test_answer(test_data, test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ceee7c-2cce-42bc-be18-bac26744f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fadd728d-4462-46ec-a95b-76488fc63f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classify(read_data_train):\n",
    "    Result=[]\n",
    "    count=0\n",
    "    for i in range(len(read_data_train)):\n",
    "        sentences = re.findall(r'<s>(.*?)</s>', read_data_train[i][0])\n",
    "        #print(sentences[0])\n",
    "        count=0\n",
    "        #print(len(read_data_train))\n",
    "        vocabulary = sentences[0].split(\" \")\n",
    "        #print(len(vocabulary))\n",
    "        #print(vocabulary[5])\n",
    "        #print(len(read_data_train))\n",
    "        #print(read_data_train[2][1])\n",
    "\n",
    "        for j in range(len(vocabulary)):\n",
    "            if vocabulary[j] != \"\":\n",
    "                #print(vocabulary[j], end=\" \")\n",
    "                if vocabulary[j] == read_data_train[i][2]:\n",
    "                    #print(vocabulary[j] , \"  YYY  \", read_data_train[i][2],end=\" \")\n",
    "                    count += 1\n",
    "        if count >= 1:      # QA問答文本中，含有答案超過XX次以上的list\n",
    "            Result.append([read_data_train[i][0], read_data_train[i][1], read_data_train[i][2]])\n",
    "\n",
    "    print(count)\n",
    "    print(Result)           # 文本中，含有答案超過XX次以上的list\n",
    "    \n",
    "classify(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c8eb4-5cb1-4ab4-90fe-b322e536505c",
   "metadata": {},
   "source": [
    "# 用 TF-IDF or BM25 選取和問題最相近的句子，包含答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d428cfe-086d-41f7-85c3-3347058aa96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_articles(query, docs, k=1):\n",
    "\n",
    "    # Initialize a vectorizer that removes English stop words\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words='english')\n",
    "\n",
    "    # Create a corpus of query and documents and convert to TFIDF vectors\n",
    "    query_and_docs = [query] + docs\n",
    "    matrix = vectorizer.fit_transform(query_and_docs)\n",
    "\n",
    "    # Holds our cosine similarity scores\n",
    "    scores = []\n",
    "\n",
    "    # The first vector is our query text, so compute the similarity of our query against all document vectors\n",
    "    for i in range(1, len(query_and_docs)):\n",
    "        scores.append(cosine_similarity(matrix[0], matrix[i])[0][0])\n",
    "\n",
    "    # Sort list of scores and return the top k highest scoring documents\n",
    "    sorted_list = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    top_doc_indices = [x[0] for x in sorted_list[:k]]\n",
    "    top_docs = [docs[x] for x in top_doc_indices]\n",
    "  \n",
    "    return top_docs\n",
    "\n",
    "def get_topk_tfidf_sentence(data):\n",
    "    contexts = []\n",
    "    for sentences, question, answer in tqdm(data):\n",
    "        contained_answer_sentence = []\n",
    "        sentences = re.findall(r'<s>(.*?)</s>', sentences)\n",
    "        for sentence in sentences:\n",
    "            if answer in sentence:\n",
    "                contained_answer_sentence.append(sentence)\n",
    "        doc = get_top_k_articles(question, contained_answer_sentence)\n",
    "        context = \". \".join(doc)\n",
    "        contexts.append(context)\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54cf77cd-d710-4650-b7a4-a78f56b2511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_bm25_sentence(data, k):\n",
    "    contexts = []\n",
    "    for sentences, question, answer in tqdm(data):\n",
    "        contained_answer_sentence = []\n",
    "        sentences = re.findall(r'<s>(.*?)</s>', sentences)\n",
    "        for sentence in sentences:\n",
    "            if answer in sentence:\n",
    "                contained_answer_sentence.append(sentence)\n",
    "        tokenized_corpus = [doc.lower().split(\" \") for doc in contained_answer_sentence]\n",
    "        FASTBM25 = fastbm25(tokenized_corpus)\n",
    "        tokenized_answer = question.lower().split(\" \")\n",
    "        doc = FASTBM25.top_k_sentence(tokenized_answer, k=k)\n",
    "        final_doc = []\n",
    "        for list_doc, _,score in doc:\n",
    "            final_doc.append(\" \".join(list_doc))\n",
    "        context = \". \".join(final_doc)\n",
    "        contexts.append(context)\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3735bbc5-992a-4af3-a1de-19219fd49fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method Type: BM25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 27248/27248 [00:32<00:00, 846.26it/s]\n"
     ]
    }
   ],
   "source": [
    "if MODEL_PATH[:-1] == \"model/QA_modelBM25_k_\":\n",
    "    print('Method Type: BM25')\n",
    "    #train_contexts = get_topk_bm25_sentence(train_data, K)\n",
    "    #valid_contexts = get_topk_bm25_sentence(valid_data, K)\n",
    "    test_contexts  = get_topk_bm25_sentence(test_data,  K)\n",
    "elif MODEL_PATH[:-1] == \"model/QA_model_TFIDF_k_\":\n",
    "    print('Method Type: TF-IDF')\n",
    "    #train_contexts = get_topk_tfidf_sentence(train_data)\n",
    "    #val_contexts =   get_topk_tfidf_sentence(valid_data)\n",
    "    test_contexts  = get_topk_tfidf_sentence(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45e935-3121-4efc-9239-2a6d8ddeae7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 轉換成Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b22824-930b-4364-b439-1d6949bf00e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27217\n"
     ]
    }
   ],
   "source": [
    "def convert_to_dataframe(data, contexts):\n",
    "    ques = []\n",
    "    ans= []\n",
    "    for sentences, question, answer in data:\n",
    "        ques.append(question)\n",
    "        ans.append(answer)\n",
    "    return pd.DataFrame({'sentences':contexts,'question':ques,'answer':ans})\n",
    "\n",
    "def clean_answer_not_in_context(data):\n",
    "    index_ids=[]\n",
    "    for i in range(len(data)):\n",
    "        if data['answer'].iloc[i] in data['sentences'].iloc[i]:\n",
    "            continue\n",
    "        else:\n",
    "            index_ids.append(i)\n",
    "    data = data.drop(index_ids, axis=0)\n",
    "    print(len(data))\n",
    "    return data\n",
    "\n",
    "#train_data = convert_to_dataframe(train_data, train_contexts)\n",
    "#valid_data = convert_to_dataframe(valid_data, valid_contexts)\n",
    "test_data =  convert_to_dataframe(test_data, test_contexts)\n",
    "\n",
    "#train_data = clean_answer_not_in_context(train_data)\n",
    "#valid_data = clean_answer_not_in_context(valid_data)\n",
    "test_data  = clean_answer_not_in_context(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1445d-7837-4526-a51e-a1b46f8d37a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d5702f7-78e5-4352-8b60-e6237e434409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def get_tokennize_list(data):\n",
    "    data_question = data['question'].tolist()\n",
    "    data_context = data['sentences'].tolist()\n",
    "    return data_question, data_context\n",
    "\n",
    "#train_question, train_context = get_tokennize_list(train_data)\n",
    "#valid_question, valid_context = get_tokennize_list(valid_data)\n",
    "test_question,  test_context  = get_tokennize_list(test_data)\n",
    "\n",
    "#train_encodings = tokenizer(train_question, train_context, truncation=True, padding=True)\n",
    "#val_encodings   = tokenizer(valid_question, valid_context, truncation=True, padding=True)\n",
    "test_encodings  = tokenizer(test_question, test_context, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8ab73-cda1-4f54-876a-8a03be778328",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 找答案的start end index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaf0d315-ac97-4b61-905a-1401f3d6a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Start_End_index(data):\n",
    "    data['start'] = [y.index(x) for x,y in zip(data[\"answer\"],data[\"sentences\"])]\n",
    "    data['end']   = [x+len(str(y)) for x,y in zip(data[\"start\"],data[\"answer\"])]\n",
    "#get_Start_End_index(train_data)\n",
    "#get_Start_End_index(valid_data)\n",
    "get_Start_End_index(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497799e4-ac87-4d75-b9ef-58e34bafc684",
   "metadata": {
    "tags": []
   },
   "source": [
    "# add token positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c9cf6e-3f34-4cac-a7fb-4c12eb74ca1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start', 'end'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_answer = train_data[['start', 'end']].to_dict('records')\n",
    "#valid_answer = valid_data[['start', 'end']].to_dict('records')\n",
    "test_answer  = test_data[['start', 'end']].to_dict('records')\n",
    "\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['start'],1))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['end']-1,1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "            \n",
    "        shift = 1\n",
    "        while end_positions[-1] is None:\n",
    "            if answers[i]['end'] - shift>=0:\n",
    "                end_positions[-1] = encodings.char_to_token(i, answers[i]['end'] - shift, 1)\n",
    "                shift += 1 \n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    encodings.update({'start': start_positions, 'end': end_positions})\n",
    "\n",
    "#add_token_positions(train_encodings, train_answer)\n",
    "#add_token_positions(val_encodings, valid_answer)\n",
    "add_token_positions(test_encodings, test_answer)\n",
    "\n",
    "test_encodings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381f5a8-6242-47ed-a3cb-b05c5e3407aa",
   "metadata": {},
   "source": [
    "# convert to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76557abc-b0a1-4f14-9062-1adeb15ba9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            return {key: torch.tensor(value[idx]) for key, value in self.encodings.items()}\n",
    "        except:\n",
    "            print(idx)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "#train_dataset = QADataset(train_encodings)\n",
    "#val_dataset   = QADataset(val_encodings)\n",
    "test_dataset  = QADataset(test_encodings)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606ffe27-ad56-4cff-b8f7-6ac579eb5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack data into dataloader by batch\n",
    "batch_size   = 4\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674f8be-152b-4b8c-9377-fc6491c55358",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d7b0ddb-6819-4ae5-9446-d18b50cea7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class QAModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(QAModel, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
    "        logits = output[0]\n",
    "        out = self.fc(logits)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0acf7597-0e1d-4e17-b6b1-a7640efaa198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QAModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put model on device\n",
    "model = QAModel().to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f7e807-8d13-4143-a9a1-c4ed5ea0f337",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfb31815-0108-4a1c-9883-7b21950ad954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predict_pos = []\n",
    "    sub_output = []\n",
    "\n",
    "    loop = tqdm(test_loader, leave=True)\n",
    "    for batch_id, batch in enumerate(loop):\n",
    "        input_ids      = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "        # model output\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        start_logits, end_logits = torch.split(outputs, 1, 2)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits   = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        start_prdict = torch.argmax(start_logits, 1).cpu().numpy()\n",
    "        end_prdict   = torch.argmax(end_logits, 1).cpu().numpy()\n",
    "\n",
    "        for i in range(len(input_ids)):\n",
    "            predict_pos.append((start_prdict[i].item(), end_prdict[i].item()))\n",
    "            sub = tokenizer.decode(input_ids[i][start_prdict[i]:end_prdict[i]+1])\n",
    "            sub_output.append(sub)\n",
    "    \n",
    "    return sub_output, predict_pos\n",
    "\n",
    "def nltk_token_string(sentence):\n",
    "    # print(sentence)\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        if len(tokens[i]) == 1:\n",
    "            tokens[i] = re.sub(r\"[!\\\"#$%&\\'()*\\+, -.\\/:;<=>?@\\[\\\\\\]^_`{|}~]\", '', tokens[i])\n",
    "    while '' in tokens:\n",
    "        tokens.remove('')\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def get_output_post_fn(test, sub_output):\n",
    "    sub = []\n",
    "    for i in range(len(test)):\n",
    "\n",
    "        sub_pred = sub_output[i].split()\n",
    "\n",
    "        temp = sub_pred.copy()\n",
    "        if sub_pred is None:\n",
    "            sub_pred = []\n",
    "        else:\n",
    "            for j in range(len(temp)):\n",
    "                if temp[j] == '[SEP]':\n",
    "                    sub_pred.remove('[SEP]')\n",
    "                if temp[j] == '[PAD]':\n",
    "                    sub_pred.remove('[PAD]')\n",
    "\n",
    "        sub.append(' '.join(sub_pred))\n",
    "        \n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71ff3961-2b47-4af0-a55f-5df0c03e6600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 6805/6805 [11:33<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "england\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>difference united kingdom , great britain , e...</td>\n",
       "      <td>'s largest kingdom united kingdom</td>\n",
       "      <td>england</td>\n",
       "      <td>45</td>\n",
       "      <td>52</td>\n",
       "      <td>england</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>back rockville review miley cyrus oct 18 , 20...</td>\n",
       "      <td>party u singer also plays young lady named hannah</td>\n",
       "      <td>miley cyrus</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>miley cyrus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>peach tree fruit britannica com mar 4 , 2015 ...</td>\n",
       "      <td>part peach downy fuzzy , fruit 's called peach...</td>\n",
       "      <td>skin</td>\n",
       "      <td>157</td>\n",
       "      <td>161</td>\n",
       "      <td>peaches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cincinnati 5fl14 3 4 x 12'hydraulic shear 168...</td>\n",
       "      <td>4 x 12</td>\n",
       "      <td>48</td>\n",
       "      <td>176</td>\n",
       "      <td>178</td>\n",
       "      <td>cincinnati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grammarphobia blog dribbling , court bib apr ...</td>\n",
       "      <td>verb bouncing basketball sounds like 're slobb...</td>\n",
       "      <td>dribbling</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>dribbling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chfpatients com heart failure faq ejection fr...</td>\n",
       "      <td>blood pumper</td>\n",
       "      <td>heart</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>arteries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>download new catalog ati courses space , sate...</td>\n",
       "      <td>sound navigation ranging full name device boun...</td>\n",
       "      <td>sonar</td>\n",
       "      <td>253</td>\n",
       "      <td>258</td>\n",
       "      <td>sonar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dart definition dart free dictionary slender ...</td>\n",
       "      <td>small , slender missile thrown board game</td>\n",
       "      <td>dart</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>dart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gemstone jewelry glossary terms jewelry telev...</td>\n",
       "      <td>5 letter word hard interior peach</td>\n",
       "      <td>stone</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>old fashioned ball games free software sharew...</td>\n",
       "      <td>kid 's game , bounce small rubber ball picking...</td>\n",
       "      <td>jacks</td>\n",
       "      <td>119</td>\n",
       "      <td>124</td>\n",
       "      <td>jacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>part hair 9 steps pictures wikihow hair parti...</td>\n",
       "      <td>separating line hair role play</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>underground fossil water running national geo...</td>\n",
       "      <td>parts arabian libyan deserts found african cou...</td>\n",
       "      <td>egypt</td>\n",
       "      <td>156</td>\n",
       "      <td>161</td>\n",
       "      <td>egypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>garden plants poisonous people nsw department...</td>\n",
       "      <td>parts peach tree glossy green , pointed lance ...</td>\n",
       "      <td>leaves</td>\n",
       "      <td>119</td>\n",
       "      <td>125</td>\n",
       "      <td>leaves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>moonwalk heroku moonwalk dance wikipedia free...</td>\n",
       "      <td>'s type bounce house , dance made famous micha...</td>\n",
       "      <td>moonwalk</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>moonwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>graphic representation information system cal...</td>\n",
       "      <td>graphic representation information</td>\n",
       "      <td>chart</td>\n",
       "      <td>104</td>\n",
       "      <td>109</td>\n",
       "      <td>histogram e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>99 03 06 italian immigrant experience america...</td>\n",
       "      <td>family history wrote school might include ente...</td>\n",
       "      <td>ellis island</td>\n",
       "      <td>95</td>\n",
       "      <td>107</td>\n",
       "      <td>ellis island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>download onerepublic right moves mp3 hits son...</td>\n",
       "      <td>lead singer ryan tedder band right moves</td>\n",
       "      <td>onerepublic</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>onerepublic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>wolfgang amadeus mozart find mozart , compose...</td>\n",
       "      <td>composer wolfgang</td>\n",
       "      <td>mozart</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>amadeus mozart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>chp 6 1 atoms atom 's mass 99 95 contained ti...</td>\n",
       "      <td>99 95 mass atom part</td>\n",
       "      <td>nucleus</td>\n",
       "      <td>60</td>\n",
       "      <td>67</td>\n",
       "      <td>nucleus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>magic behind harry potter cbs news harry pott...</td>\n",
       "      <td>hero several books 11 discovers 's wizard</td>\n",
       "      <td>harry potter</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>harry potter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentences  \\\n",
       "0    difference united kingdom , great britain , e...   \n",
       "1    back rockville review miley cyrus oct 18 , 20...   \n",
       "2    peach tree fruit britannica com mar 4 , 2015 ...   \n",
       "3    cincinnati 5fl14 3 4 x 12'hydraulic shear 168...   \n",
       "4    grammarphobia blog dribbling , court bib apr ...   \n",
       "5    chfpatients com heart failure faq ejection fr...   \n",
       "6    download new catalog ati courses space , sate...   \n",
       "7    dart definition dart free dictionary slender ...   \n",
       "8    gemstone jewelry glossary terms jewelry telev...   \n",
       "9    old fashioned ball games free software sharew...   \n",
       "10   part hair 9 steps pictures wikihow hair parti...   \n",
       "11   underground fossil water running national geo...   \n",
       "12   garden plants poisonous people nsw department...   \n",
       "13   moonwalk heroku moonwalk dance wikipedia free...   \n",
       "14   graphic representation information system cal...   \n",
       "15   99 03 06 italian immigrant experience america...   \n",
       "16   download onerepublic right moves mp3 hits son...   \n",
       "17   wolfgang amadeus mozart find mozart , compose...   \n",
       "18   chp 6 1 atoms atom 's mass 99 95 contained ti...   \n",
       "19   magic behind harry potter cbs news harry pott...   \n",
       "\n",
       "                                             question        answer  start  \\\n",
       "0                   's largest kingdom united kingdom       england     45   \n",
       "1   party u singer also plays young lady named hannah   miley cyrus     23   \n",
       "2   part peach downy fuzzy , fruit 's called peach...          skin    157   \n",
       "3                                              4 x 12            48    176   \n",
       "4   verb bouncing basketball sounds like 're slobb...     dribbling     20   \n",
       "5                                        blood pumper         heart     17   \n",
       "6   sound navigation ranging full name device boun...         sonar    253   \n",
       "7           small , slender missile thrown board game          dart      1   \n",
       "8                   5 letter word hard interior peach         stone      4   \n",
       "9   kid 's game , bounce small rubber ball picking...         jacks    119   \n",
       "10                     separating line hair role play          part      1   \n",
       "11  parts arabian libyan deserts found african cou...         egypt    156   \n",
       "12  parts peach tree glossy green , pointed lance ...        leaves    119   \n",
       "13  's type bounce house , dance made famous micha...      moonwalk      1   \n",
       "14                 graphic representation information         chart    104   \n",
       "15  family history wrote school might include ente...  ellis island     95   \n",
       "16           lead singer ryan tedder band right moves   onerepublic     10   \n",
       "17                                  composer wolfgang        mozart     18   \n",
       "18                               99 95 mass atom part       nucleus     60   \n",
       "19          hero several books 11 discovers 's wizard  harry potter     14   \n",
       "\n",
       "    end         predict  \n",
       "0    52         england  \n",
       "1    34     miley cyrus  \n",
       "2   161         peaches  \n",
       "3   178      cincinnati  \n",
       "4    29       dribbling  \n",
       "5    22        arteries  \n",
       "6   258           sonar  \n",
       "7     5            dart  \n",
       "8     9                  \n",
       "9   124           jacks  \n",
       "10    5            part  \n",
       "11  161           egypt  \n",
       "12  125          leaves  \n",
       "13    9        moonwalk  \n",
       "14  109     histogram e  \n",
       "15  107    ellis island  \n",
       "16   21     onerepublic  \n",
       "17   24  amadeus mozart  \n",
       "18   67         nucleus  \n",
       "19   26    harry potter  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_output, predict_pos = predict(test_loader)\n",
    "print(sub_output[0])\n",
    "sub = get_output_post_fn(test_data, sub_output)\n",
    "test_data['predict'] = sub\n",
    "test_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "089a5363-26a5-4c69-bfb8-efda610e913a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/pandas/core/indexes/base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sub\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_data\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/pandas/core/frame.py:5273\u001b[0m, in \u001b[0;36mDataFrame.pop\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   5232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: Hashable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m   5233\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5234\u001b[0m \u001b[38;5;124;03m    Return item and drop from frame. Raise KeyError if not found.\u001b[39;00m\n\u001b[1;32m   5235\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5271\u001b[0m \u001b[38;5;124;03m    3  monkey        NaN\u001b[39;00m\n\u001b[1;32m   5272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/pandas/core/generic.py:865\u001b[0m, in \u001b[0;36mNDFrame.pop\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: Hashable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m Any:\n\u001b[0;32m--> 865\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m[item]\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/pandas/core/indexes/base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3633\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start'"
     ]
    }
   ],
   "source": [
    "test_data['predict'] = sub\n",
    "test_data.pop('end')\n",
    "test_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84219a4e-a5ce-475c-a05f-35b1b426d215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joeyliang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import collections\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "\n",
    "def lcs(X, Y):\n",
    "    X_, Y_ = [], []\n",
    "    \n",
    "    X_ = nltk_token_string(X)\n",
    "    Y_ = nltk_token_string(Y)\n",
    "\n",
    "    m = len(X_)\n",
    "    n = len(Y_)\n",
    " \n",
    "    # declaring the array for storing the dp values\n",
    "    L = [[None]*(n + 1) for i in range(m + 1)]\n",
    " \n",
    "    \"\"\"Following steps build L[m + 1][n + 1] in bottom up fashion\n",
    "    Note: L[i][j] contains length of LCS of X[0..i-1]\n",
    "    and Y[0..j-1]\"\"\"\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0 or j == 0 :\n",
    "                L[i][j] = 0\n",
    "            elif X_[i-1] == Y_[j-1]:\n",
    "                L[i][j] = L[i-1][j-1]+1\n",
    "            else:\n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    " \n",
    "    # L[m][n] contains the length of LCS of X[0..n-1] & Y[0..m-1]\n",
    "    return L[m][n]\n",
    "\n",
    "\n",
    "def acc(full, sub):\n",
    "    common = lcs(full, sub)\n",
    "    union = len(full) + len(sub) - common\n",
    "    accuracy = float(common/union)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def LCS(data):\n",
    "    acc_sum = 0\n",
    "    for i in range(data.shape[0]):\n",
    "        accuracy = acc(data.iloc[i][\"answer\"], data.iloc[i]['predict'])\n",
    "        acc_sum += accuracy\n",
    "    print(\"LCS accuracy: \", acc_sum/data.shape[0])\n",
    "    \n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  \n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "  \n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "  \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "  \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "  \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "    \n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "   \n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "  \n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def f1_metric(data):\n",
    "    f1_score = 0.0\n",
    "    for answer, pred in zip(data[\"answer\"], data['predict']):\n",
    "        f1_score+=compute_f1(answer, pred)\n",
    "    print(\"F1 score: {}\".format(f1_score/len(data)))\n",
    "    \n",
    "def EM_score(data):\n",
    "    total = len(data)\n",
    "    calculate = 0\n",
    "    for i in range(len(data)):\n",
    "        if data['answer'].iloc[i] == data['predict'].iloc[i]:\n",
    "            calculate+=1\n",
    "    print(\"EM score: {}\".format(calculate/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6b2639b-d118-4cfc-87fe-ef715ceb2248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCS accuracy:  0.8048942179111295\n",
      "F1 score: 0.7796173606178918\n",
      "EM score: 0.6975052356982768\n"
     ]
    }
   ],
   "source": [
    "LCS(test_data)\n",
    "f1_metric(test_data)\n",
    "EM_score(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
